{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "# Limitations and challenges in mobility data: skewness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Set to True to render the kepler map or altair charts when running the notebook in local\n",
    "# If False, screenshots will be shown instead\n",
    "INTERACTIVE_OUTPUT = False\n",
    "\n",
    "# If the platform allows to render gifs, set to True:\n",
    "RENDERING_GIF = False\n",
    "\n",
    "# Scaffolding to save the maps in html format:\n",
    "SAVE_HTML_MAPS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We can extract the dataset with pyspark**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, input_file_name\n",
    "\n",
    "\n",
    "def extract_and_save_to(destination: str = \"dois_in_sydney.parquet\") -> None:  # noqa: F811 redefinition\n",
    "    \"\"\"A function for databricks.\"\"\"\n",
    "\n",
    "    # move the import statements out of the function when running on databricks\n",
    "\n",
    "    file_location = \"<s3 location here>.*.csv.gz\"\n",
    "\n",
    "    # Read the data\n",
    "    df_data = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"sep\", \"|\")\n",
    "        .load(file_location)\n",
    "        .withColumn(\"filename\", input_file_name())\n",
    "    )\n",
    "    df_data = (\n",
    "        df_data.withColumnRenamed(\"_c0\", \"VRID\")\n",
    "        .withColumnRenamed(\"_c1\", \"MAID\")\n",
    "        .withColumnRenamed(\"_c2\", \"device_type\")\n",
    "        .withColumnRenamed(\"_c3\", \"timestamp\")\n",
    "        .withColumnRenamed(\"_c4\", \"time_zone\")\n",
    "        .withColumnRenamed(\"_c5\", \"lat\")\n",
    "        .withColumnRenamed(\"_c6\", \"lon\")\n",
    "    )\n",
    "    df_data = df_data.select(\n",
    "        col(\"VRID\"),\n",
    "        col(\"MAID\"),\n",
    "        col(\"device_type\"),\n",
    "        col(\"timestamp\"),\n",
    "        col(\"time_zone\"),\n",
    "        col(\"lat\"),\n",
    "        col(\"lon\"),\n",
    "    )\n",
    "\n",
    "    # take only the pings in the bbox:\n",
    "    min_lon, min_lat, max_lon, max_lat = 151.073941, -33.966366, 151.298097, -33.804344\n",
    "    df_bbox = df_data.filter(\n",
    "        (F.col(\"lon\") >= min_lon) & (F.col(\"lon\") <= max_lon) & (F.col(\"lat\") >= min_lat) & (F.col(\"lat\") <= max_lat)\n",
    "    )\n",
    "\n",
    "    # Time filter - milliseconds\n",
    "    str_min_date = \"2022-06-01 00:00:00+00:00\"\n",
    "    str_max_date = \"2022-06-30 23:59:59+00:00\"\n",
    "\n",
    "    timestamp_min_time_msec = int(dt.datetime.strptime(str_min_date, \"%Y-%m-%d %H:%M:%S%z\").timestamp()) * 1000\n",
    "    timestamp_max_time_msec = int(dt.datetime.strptime(str_max_date, \"%Y-%m-%d %H:%M:%S%z\").timestamp()) * 1000\n",
    "\n",
    "    df_bbox_tfilter = df_bbox.filter(\n",
    "        (F.col(\"timestamp\") >= timestamp_min_time_msec) & (F.col(\"timestamp\") <= timestamp_max_time_msec)\n",
    "    )\n",
    "\n",
    "    # take all the pings from devices that have appeared at least once in the bbox (devices of interests):\n",
    "    unique_devices = df_bbox_tfilter.select(\"MAID\").distinct()\n",
    "    df_data = df_data.alias(\"df_data\")\n",
    "    df_pings_of_interest = df_data.join(unique_devices, df_data.MAID == unique_devices.MAID).select(\"df_data.*\")\n",
    "\n",
    "    # Save to a parquet file for future use:\n",
    "    df_pings_of_interest.write.parquet((destination), \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Or we can extract the dataset with dfipy** (with way less number of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from dfi import Client\n",
    "\n",
    "\n",
    "def extract_and_save_to(destination: str = \"dois_in_sydney.parquet\") -> None:  # noqa: F811 redefinition\n",
    "    \"\"\"Function for any notebook with dfi credentials.\"\"\"\n",
    "\n",
    "    dfi = Client(\n",
    "        api_token=\"<api token>\",\n",
    "        namespace=\"<namespace>\",\n",
    "        instance_name=\"<instance name>\",\n",
    "        base_url=\"<base url>\",\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = 151.073941, -33.966366, 151.298097, -33.804344\n",
    "    start_time = datetime(2022, 6, 1, 24, 0, 0)\n",
    "    end_time = datetime(2022, 6, 30, 23, 59, 59)\n",
    "\n",
    "    unique_entities = dfi.get.entities(\n",
    "        geometry=[min_lon, min_lat, max_lon, max_lat],\n",
    "        time_interval=(start_time, end_time),\n",
    "    )\n",
    "\n",
    "    df_doi = dfi.get.records(entities=unique_entities)\n",
    "\n",
    "    df_doi.to_parquet(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTIVE_OUTPUT:\n",
    "    # Run time ~ 3 minutes\n",
    "    from copy import deepcopy\n",
    "\n",
    "    import geopandas as gpd\n",
    "    import h3\n",
    "    import pandas as pd\n",
    "    from keplergl import KeplerGl\n",
    "    from shapely.geometry import Polygon\n",
    "\n",
    "    # after running extract_and_save_to on the selected dataset:\n",
    "    df = pd.read_parquet(\"dois_in_sydney.parquet\")\n",
    "\n",
    "    df[\"hex_id\"] = df.apply(lambda row: h3.geo_to_h3(row[\"lat\"], row[\"lon\"], 8), axis=1)\n",
    "    df_hex_bin = pd.DataFrame({\"num_pings\": df.groupby(\"hex_id\").size()}).reset_index(drop=False)\n",
    "\n",
    "    geometries = [Polygon(h3.h3_to_geo_boundary(idx, geo_json=True)) for idx in df_hex_bin[\"hex_id\"]]\n",
    "    gdf_grouped = gpd.GeoDataFrame(df_hex_bin, geometry=geometries)[[\"num_pings\", \"geometry\"]]\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = 151.073941, -33.966366, 151.298097, -33.804344\n",
    "\n",
    "    gdf_bbox = gpd.GeoDataFrame(\n",
    "        geometry=[\n",
    "            Polygon(\n",
    "                (\n",
    "                    (max_lon, max_lat),\n",
    "                    (max_lon, min_lat),\n",
    "                    (min_lon, min_lat),\n",
    "                    (min_lon, max_lat),\n",
    "                    (max_lon, max_lat),\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    from kepler_configs import config_heatmap\n",
    "\n",
    "    kmap = KeplerGl(\n",
    "        data={\"hexes\": deepcopy(gdf_grouped), \"bbox\": deepcopy(gdf_bbox)},\n",
    "        height=1200,\n",
    "        config=config_heatmap,\n",
    "    )\n",
    "\n",
    "    if SAVE_HTML_MAPS:\n",
    "        kmap.save_to_html(\n",
    "            config=kmap.config.copy(),\n",
    "            file_name=\"maps/skewness_0_data_presentation.html\",\n",
    "        )\n",
    "\n",
    "else:\n",
    "    display(Image(\"images/skewness_0_data_presentation.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTIVE_OUTPUT:\n",
    "    import altair as alt\n",
    "    import numpy as np\n",
    "\n",
    "    df_pings_per_device = pd.DataFrame(df.groupby(\"MAID\").size()).reset_index().rename(columns={0: \"num\"})\n",
    "    df_pings_per_device.head()\n",
    "\n",
    "    bins = np.arange(2, df_pings_per_device.num.max(), df_pings_per_device.num.max() / 100)\n",
    "    binned = (\n",
    "        df_pings_per_device.groupby(pd.cut(df_pings_per_device.num, bins=bins)).count()[[\"MAID\"]].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    binned[\"bin_min\"] = bins[:-1]\n",
    "    binned[\"bin_max\"] = bins[1:]\n",
    "\n",
    "    alt.themes.enable(\"dark\")\n",
    "\n",
    "    chart = (\n",
    "        alt.Chart(binned, title=\"Number of pings per device\")\n",
    "        .mark_bar()\n",
    "        .encode(\n",
    "            x=alt.X(\"bin_min\", bin=\"binned\", title=\"number of pings\"),\n",
    "            x2=\"bin_max\",\n",
    "            y=alt.Y(\n",
    "                \"MAID\",\n",
    "                scale=alt.Scale(type=\"symlog\"),\n",
    "                title=\"number of devices per pings\",\n",
    "            ),\n",
    "        )\n",
    "        .properties(width=700, height=800)\n",
    "        .configure_axis(\n",
    "            labelFontSize=20,\n",
    "            titleFontSize=20,\n",
    "        )\n",
    "        .configure_title(\n",
    "            fontSize=20,\n",
    "            anchor=\"start\",\n",
    "        )\n",
    "        .interactive(True)\n",
    "    )\n",
    "\n",
    "    display(chart)\n",
    "\n",
    "else:\n",
    "    display(Image(\"images/skewness_1_pings_per_device.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
