{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dfipy` Quick Start Guide - Large Synthetic Dataset of 92B Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through querying a large synthetic traffic dataset in the General System Platform.\n",
    "\n",
    "Please refer to our [developers guide](https://developers.generalsystem.com) for the most up-to-date companion documentation.\n",
    "\n",
    "Additional resources and help are available on the [General System support pages](https://support.generalsystem.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, this will set up all required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from getpass import getpass\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Google Colab setup\n",
    "try:\n",
    "    from google.colab import output\n",
    "\n",
    "    output.enable_custom_widget_manager()  # allows KeplerGL map to display\n",
    "\n",
    "    ! pip install dfipy==9.0.1 h3==3.7.6 keplergl==0.3.2\n",
    "\n",
    "    import dfi.models.filters.geometry as geom\n",
    "    import h3\n",
    "    from dfi import Client\n",
    "    from dfi.models.filters import TimeRange\n",
    "    from keplergl import KeplerGl\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    import dfi.models.filters.geometry as geom\n",
    "    import h3\n",
    "    from dfi import Client\n",
    "    from dfi.models.filters import TimeRange\n",
    "    from keplergl import KeplerGl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs_url = (\n",
    "    \"https://raw.githubusercontent.com/thegeneralsystem/dfipy-examples/main/examples/datasets/london_boroughs.json\"\n",
    ")\n",
    "congestion_zone_url = \"https://raw.githubusercontent.com/thegeneralsystem/dfipy-examples/main/examples/datasets/london_congestion_zone.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, enter you API access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = getpass(\"Enter your API access token: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial we will be querying a large synthetic data set\n",
    "\n",
    "This synthetic data set represent traffic moving across London.\n",
    "\n",
    "| total records\t| 92,435,312,835 |\n",
    "| ------------- | -------------- |\n",
    "| distinct uuids | 1,578,544 |\n",
    "| start time | 2022-01-01 00:00:00 |\n",
    "| end time | 2022-08-26 07:12:00 |\n",
    "\n",
    "Bounding box of all data:\n",
    "\n",
    "|      | Longitude  | Latitude |\n",
    "| ---- | ---------- | -------- |\n",
    "| Min  | -0.5120832 | 51.2810883 |\n",
    "| Max  | 0.322123   | 51.6925997 |\n",
    "\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- The dataset runs on a single server hosted on AWS\n",
    "- The server is storage optimised, with 192GB ram and 2 x 7.5TB NVMe SSD\n",
    "\n",
    "#### Note: this is a shared instance, and you cannot add or delete data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture shows a heatmap of the London Traffic dataset of 92bn records.\n",
    "\n",
    "The data distributes along the roads in the city, with Central London having the largest density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = (\n",
    "    \"https://raw.githubusercontent.com/thegeneralsystem/dfipy-examples/main/examples/pictures/london_traffic.jpg\"\n",
    ")\n",
    "Image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the DFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.aspen.generalsystem.com\"\n",
    "dataset_id = \"gs.prod-3\"\n",
    "\n",
    "dfi = Client(\n",
    "    api_token=api_token,\n",
    "    base_url=base_url,\n",
    "    progress_bar=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define auxiliary methods to aggregate and analyse the datasets.\n",
    "\n",
    "In what follows, we use H3 spatial indices to aggregate large amounts of data. \n",
    "\n",
    "An H3 spatial index allows for efficient spatial referencing, indexing, and analysis of geospatial data at varying resolutions on a global scale. `h3_resolution` refers to the granularity of the aggregation, with higher values providing finer resolution. For more information please see: https://h3geo.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_lat_lon(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Explodes the 'coordinate' column into separate columns for longitude and latitude\"\"\"\n",
    "    df_exploded = df.copy()\n",
    "    df_exploded[[\"longitude\", \"latitude\", \"altitude\"]] = pd.DataFrame(\n",
    "        df_exploded[\"coordinate\"].tolist(), index=df_exploded.index\n",
    "    )\n",
    "    df_exploded.drop(columns=[\"coordinate\", \"altitude\"], inplace=True)\n",
    "    return df_exploded\n",
    "\n",
    "\n",
    "def _aggregate_records(df_input: pd.DataFrame, hex_id: str) -> pd.DataFrame:\n",
    "    return (\n",
    "        df_input.groupby(hex_id)\n",
    "        .agg(\n",
    "            num_records=(\"id\", \"count\"),\n",
    "            num_devices=(\"id\", \"nunique\"),\n",
    "            first_ping=(\"time\", \"min\"),\n",
    "            last_ping=(\"time\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "def add_heatmap_aggregation(\n",
    "    df_records: pd.DataFrame,\n",
    "    h3_resolution: int,\n",
    ") -> pd.DataFrame:\n",
    "    return df_records.assign(\n",
    "        hex_id=lambda df: [\n",
    "            h3.geo_to_h3(lat, lon, resolution=int(h3_resolution)) for lat, lon in zip(df[\"latitude\"], df[\"longitude\"])\n",
    "        ]\n",
    "    ).pipe(_aggregate_records, \"hex_id\")\n",
    "\n",
    "\n",
    "def build_heatmap(df_records: pd.DataFrame, h3_resolution: int) -> gpd.GeoDataFrame:\n",
    "    df_binned_data = add_heatmap_aggregation(df_records=df_records, h3_resolution=h3_resolution)\n",
    "    hex_geometries = [Polygon(h3.h3_to_geo_boundary(h3_id, geo_json=True)) for h3_id in df_binned_data.hex_id]\n",
    "    gdf_binned_data = gpd.GeoDataFrame(df_binned_data, geometry=hex_geometries)\n",
    "\n",
    "    gdf_binned_data_kepler = gdf_binned_data.copy()\n",
    "    gdf_binned_data_kepler = gdf_binned_data_kepler.drop(columns=[\"hex_id\"])\n",
    "    gdf_binned_data_kepler.first_ping = gdf_binned_data_kepler.first_ping.astype(str)\n",
    "    gdf_binned_data_kepler.last_ping = gdf_binned_data_kepler.last_ping.astype(str)\n",
    "    gdf_binned_data_kepler = gdf_binned_data_kepler.drop(columns=[\"first_ping\", \"last_ping\"])\n",
    "    return gdf_binned_data_kepler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs_request = requests.get(boroughs_url, timeout=30)\n",
    "boroughs_request.raise_for_status()\n",
    "boroughs = boroughs_request.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, vertices in boroughs.items():\n",
    "    print(f\"{name} - {len(vertices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the polygons on a map. As an example, we plot the polygon that defines the London Congestion Zone in Central London.\n",
    "\n",
    "For visualisation we use KeplerGL: a powerful data visualisation software that enables users to explore and analyse large datasets in a visually engaging and intuitive manner. We will be utilising it to visualise the data retrieved to make it easier to share insights gained by DFI. For more information please see: https://kepler.gl/.\n",
    "\n",
    "An H3 spatial index allows for efficient spatial referencing, indexing, and analysis of geospatial data at varying resolutions on a global scale. `h3_resolution` refers to the granularity of the aggregation, with higher values providing finer resolution. For more information please see: https://h3geo.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_map(\n",
    "    list_polygons: Optional[List[List[List[float]]]] = None,\n",
    "    list_dfs: Optional[Union[gpd.GeoDataFrame, pd.DataFrame]] = None,\n",
    "    df_records: Optional[pd.DataFrame] = None,\n",
    "    map_height: int = 1200,\n",
    "    config: Optional[dict] = None,\n",
    ") -> KeplerGl:\n",
    "    if list_polygons is None:\n",
    "        list_polygons = []\n",
    "\n",
    "    dict_polygons = {f\"polygon {idx}\": poly for idx, poly in enumerate(list_polygons)}\n",
    "\n",
    "    kepler_data = {}\n",
    "\n",
    "    if len(dict_polygons) > 0:\n",
    "        kepler_data.update(\n",
    "            {\n",
    "                \"polygons\": gpd.GeoDataFrame(\n",
    "                    dict_polygons.keys(),\n",
    "                    geometry=[Polygon(x) for x in dict_polygons.values()],\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if df_records is not None:\n",
    "        kepler_data.update({\"records\": df_records.copy()})\n",
    "\n",
    "    if list_dfs is not None:\n",
    "        for idx, df in enumerate(list_dfs):\n",
    "            kepler_data.update({f\"df_{idx}\": df.copy()})\n",
    "\n",
    "    if config is None:\n",
    "        return KeplerGl(data=deepcopy(kepler_data), height=map_height)\n",
    "    return KeplerGl(data=deepcopy(kepler_data), height=map_height, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congestion_zone_request = requests.get(congestion_zone_url, timeout=30)\n",
    "congestion_zone_request.raise_for_status()\n",
    "lon_congestion_zone = congestion_zone_request.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_map(map_height=400, list_polygons=[lon_congestion_zone])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the London traffic dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is a large (92bn) dataset. While it is possible to run queries such as \"return all records in this dataset\", such queries will be streaming back large amounts of data and will be terminated early to preserve resource in the demo instance. Please include a time interval in your queries to reduce the amount of data streamed back.\n",
    "\n",
    "Querying spatiotemporal data typically takes hours or days, especially with a point-in-polygon query like this.\n",
    "\n",
    "Letâ€™s check how many vehicles entered the London Congestion Charging zone during the morning rush hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = TimeRange().from_strings(min_time=\"2022-01-01T08:00:00+00:00\", max_time=\"2022-01-01T09:30:00+00:00\")\n",
    "congestion_zone_polygon = geom.Polygon().from_raw_coords(coordinates=lon_congestion_zone, geojson=True)\n",
    "\n",
    "df_congestion_charge_zone = dfi.query.records(\n",
    "    dataset_id=\"gs.prod-3\", geometry=congestion_zone_polygon, time_range=time_range\n",
    ")\n",
    "print(f\"Records downloaded: {len(df_congestion_charge_zone):,}\")\n",
    "print(f\"Vehicles found: {len(df_congestion_charge_zone.id.unique()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a heatmap of the data returned and visualise it on a map\n",
    "\n",
    "- Cells with darker colours represent areas with less density of records\n",
    "- Cells with ligther colours represent areas with higher density of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load map configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/thegeneralsystem/dfipy-examples/main/examples/kepler_config/syn_traffic.json\"\n",
    "response = requests.get(url, timeout=30)\n",
    "kepler_config = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_congestion_charge_zone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latlons = explode_lat_lon(df=df_congestion_charge_zone)\n",
    "heatmap = build_heatmap(df_records=df_latlons, h3_resolution=10)\n",
    "map1 = show_map(map_height=400, list_dfs=[heatmap], config=kepler_config)\n",
    "map1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the full history of a vehicle and show it on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = \"a37d6189-00ed-4f45-bb6e-aacb1d85090e\"\n",
    "df_history = dfi.query.records(dataset_id=\"gs.prod-3\", uids=[vehicle])\n",
    "df_latlons = explode_lat_lon(df=df_history)\n",
    "show_map(\n",
    "    df_records=df_latlons,\n",
    "    map_height=400,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Boroughs has this vehicle been to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we query each borough to check if the vehicle has visited it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfi.conn.progress_bar = False\n",
    "\n",
    "visited_boroughs = []\n",
    "for name, vertices in tqdm(boroughs.items()):\n",
    "    borough_polygon = geom.Polygon().from_raw_coords(coordinates=vertices, geojson=True)\n",
    "    records = dfi.query.records(dataset_id=\"gs.prod-3\", geometry=borough_polygon, uids=[vehicle])\n",
    "    count = len(records)\n",
    "    if count > 0:\n",
    "        visited_boroughs.append([name])\n",
    "\n",
    "dfi.conn.progress_bar = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{vehicle} has been to {len(visited_boroughs)} / {len(boroughs)} London boroughs\")\n",
    "print(\"Borough visited:\")\n",
    "for borough in visited_boroughs:\n",
    "    print(borough)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
